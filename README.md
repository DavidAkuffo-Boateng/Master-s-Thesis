This repository contains codes and materials related to my MSc. Artificial Intelligence dissertation at the University of Stirling. The project investigates how compact language models perform on empathetic dialogue generation tasks, with a particular focus on data preprocessing, model efficiency, and contextual relevance.

The study compares two pre trained transformer based models, GPT 2 Small and DistilGPT, using the Empathetic Dialogues dataset. While this dataset has become a common benchmark for evaluating emotional responses in language models, existing work provides limited documentation on effective preprocessing strategies. This project addresses that gap by developing and applying a tailored preprocessing pipeline and evaluating its impact on model performance.
